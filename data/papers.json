{
  "version": "1.0",
  "exportDate": "2025-08-12T07:01:56.760Z",
  "totalPapers": 15,
  "papers": [
    {
      "id": "paper_1754981943160_n52kvvt6p",
      "title": "2013 - Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments",
      "authors": [
        "Impairment",
        "Rehabilitation C"
      ],
      "year": 2013,
      "researchArea": "Accessible Interaction",
      "keywords": [
        "Touchscreen",
        "motor   impairments",
        "physical   disabilities",
        "assistive technology",
        "YouTube",
        "iPad",
        "iPhone.  ACM Classification Keywords  H."
      ],
      "citations": 0,
      "abstract": "Analyzing User-Generated YouTube Videos to Understand Touchscreen Use by People with Motor Impairments  Lisa Anthony  UMBC Information Systems 1000 Hilltop Circle Baltimore MD 21250 USA lanthony@umbc.edu  YooJin Kim  College of Information Studies University of Maryland College Park MD 20742 USA yki...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981943160_n52kvvt6p.json"
    },
    {
      "id": "paper_1754981952582_90bo044k6",
      "title": "2016 - Interaction gestures for a wearable device defined by visually impaired children",
      "authors": [],
      "year": 2016,
      "researchArea": "Accessible Interaction",
      "keywords": [
        "Micro gestures",
        "wearables",
        "visual impairment",
        "co- design.  ACM Classification Keywords  H."
      ],
      "citations": 0,
      "abstract": "This paper reports results from two workshops organized with children with visual impairments ranging from blindness to low vision. The aim of the workshops was to suggest gestures for the interaction with a small wearable audio-bracelet. Results show a preference for mechanical buttons and touch-based gestures on the device (mainly tapping and sliding), while only one tentative suggestion is made for a gesture made with the device (shaking).  Author",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981952582_90bo044k6.json"
    },
    {
      "id": "paper_1754981960622_bokzji1u1",
      "title": "2020 - Grasp posture control of wearable extra robotic fingers with flex sensors based on neural network",
      "authors": [
        "Biomechanics",
        "Biomaterials",
        "Biomechantronics",
        "Biosignal"
      ],
      "year": 2020,
      "researchArea": "Accessible Interaction",
      "keywords": [
        "data-driven control method",
        "extra robotic fingers",
        "flex sensor",
        "force-sensitive resistor",
        "neural network",
        "bimanual manipulation"
      ],
      "citations": 0,
      "abstract": "electronics Article  Grasp Posture Control of Wearable Extra Robotic Fingers with Flex Sensors Based on Neural Network  Joga Dharma Setiawan   1,2 , Mochammad Ariyanto   1,2   , M. Munadi   1 , Muhammad Mutoha   1 , Adam Glowacz   3, *   and Wahyu Caesarendra   1,4, *  1   Department of Mechanical E...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981960622_bokzji1u1.json"
    },
    {
      "id": "paper_1754981985073_o89so926j",
      "title": "2013 - EarPut augmenting behind-the-ear devices for ear-based interaction",
      "authors": [
        "Concept",
        "Implementation  In"
      ],
      "year": 2013,
      "researchArea": "HCI New Wearable Devices",
      "keywords": [
        "Ear-based interaction",
        "mobile interaction",
        "eyes-free",
        "device augmentation",
        "touch",
        "multi-touch",
        "experiment  ACM Classification Keywords  H"
      ],
      "citations": 0,
      "abstract": "In this work-in-progress paper, we make a case for leveraging the unique affordances of the human ear for eyes-free, mobile interaction. We present EarPut, a novel interface concept, which instruments the ear as an interactive surface for touch-based interactions and its prototypical hardware implementation. The central idea behind EarPut is to go beyond prior work by unobtrusively augmenting a variety of accessories that are worn behind the ear, such as headsets or glasses. Results from a controlled experiment with 27 participants provide empirical evidence that people are able to target salient regions on their ear effectively and precisely. Moreover, we contribute a first, systematically derived interaction design space for ear-based interaction and a set of exemplary applications.  Author",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981985073_o89so926j.json"
    },
    {
      "id": "paper_1754981989296_f3hwcz43z",
      "title": "2014 - AnyButton unpowered, modeless and highly available mobile input using unmodified clothing buttons",
      "authors": [
        "Unpowered",
        "Modeless",
        "Highly Available Mobile Input Using Unmodified Clothing Buttons  Liwei Chan"
      ],
      "year": 2014,
      "researchArea": "HCI New Wearable Devices",
      "keywords": [],
      "citations": 0,
      "abstract": "AnyButton: Unpowered, Modeless and Highly Available Mobile Input Using Unmodified Clothing Buttons  Liwei Chan ∗   Chien-Ting Weng   †   Rong-Hao Liang   ‡   Bing-Yu Chen ∗ ∗†‡ National Taiwan University   ‡ Academia Sinica  ∗ { liweichan,robin } @ntu.edu.tw   †‡ { howieliang,ctweng } @cmlab.csie.nt...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981989296_f3hwcz43z.json"
    },
    {
      "id": "paper_1754981999088_hugk386vt",
      "title": "2014 - EarPut augmenting ear-worn devices for ear-based interaction",
      "authors": [
        "Technology",
        "Design"
      ],
      "year": 2014,
      "researchArea": "HCI New Wearable Devices",
      "keywords": [
        "Ear-based interaction",
        "ear-worn",
        "mobile interaction",
        "eyes-free",
        "device augmentation",
        "touch",
        "multi-touch.  ACM Classification Keywords  H."
      ],
      "citations": 0,
      "abstract": "ing with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  OZCHI , ’14, Dec 2-5, 2014, Sydney, Australia. Copyright c ©   2014 ACM 978-1-4503-0653-9...$15.00.  Author",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754981999088_hugk386vt.json"
    },
    {
      "id": "paper_1754982010769_i2spakp9d",
      "title": "2001 - Experiencing a presentation through a mixed reality boundary",
      "authors": [
        "Steve Benford",
        "Chris Greenhalgh  The Mixed Reality Laboratory"
      ],
      "year": 2001,
      "researchArea": "Immersive Interaction",
      "keywords": [
        "Distributed presentations",
        "mixed reality boundaries",
        "awareness",
        "spatial integration"
      ],
      "citations": 0,
      "abstract": "Experiencing a Presentation through a Mixed Reality Boundary  Boriana Koleva, Holger Schnädelbach, Steve Benford and Chris Greenhalgh  The Mixed Reality Laboratory, University of Nottingham Jubilee Campus Nottingham NG8 1BB, UK +44 115 951 4203  {bnk, hms, sdb, cmg}@cs.nott.ac.uk  ABSTRACT  We   des...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982010769_i2spakp9d.json"
    },
    {
      "id": "paper_1754982014677_2r2x2w5ls",
      "title": "2006 - Making digital shapes by hand",
      "authors": [
        "John Frazer et al."
      ],
      "year": 2006,
      "researchArea": "Immersive Interaction",
      "keywords": [],
      "citations": 0,
      "abstract": "MAKING DIGITAL SHAPES BY HAND  Steven Schkolne  steven@schkolne.com  California Institute of the Arts  SIGGRAPH Courses 2006  Interactive Shape Editing  INTRODUCTION  We do many things with our hands, but when it comes to making digital shapes, we tend to use  the mouse and the keyboard. The hand is...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982014677_2r2x2w5ls.json"
    },
    {
      "id": "paper_1754982024400_m30dckozl",
      "title": "2008 - HandNavigator hands-on interaction for desktop virtual reality",
      "authors": [
        "Virtual Reality Software",
        "Technology"
      ],
      "year": 2008,
      "researchArea": "Immersive Interaction",
      "keywords": [
        "hands",
        "interaction",
        "virtual reality  1"
      ],
      "citations": 0,
      "abstract": "HAL Id: inria-00336348 https://inria.hal.science/inria-00336348v1  Submitted on 3 Nov 2008  HAL   is   a   multi-disciplinary   open   access archive for the deposit and dissemination of sci- entific research documents, whether they are pub- lished or not.   The documents may come from teaching and ...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982024400_m30dckozl.json"
    },
    {
      "id": "paper_1754982037454_aanhp0wc3",
      "title": "2016 - WiFinger talk to your smart devices with finger-grained gesture",
      "authors": [
        "Science",
        "Technology"
      ],
      "year": 2016,
      "researchArea": "Mobile Device",
      "keywords": [
        "Wireless",
        "Micro-motion Recognition",
        "Channel State Information.  ACM Classification Keywords  H."
      ],
      "citations": 0,
      "abstract": "WiFinger: Talk to Your Smart Devices with Finger-grained Gesture  Hong Li, Wei Yang ∗  , Jianxin Wang, Yang Xu, Liusheng Huang  University of Science and Technology of China qubit@ustc.edu.cn  ABSTRACT  In recent literatures, WiFi signals have been widely used to “sense” people’s locations and activ...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982037454_aanhp0wc3.json"
    },
    {
      "id": "paper_1754982055357_y5l1k823h",
      "title": "2018 - Fallacies of agreement a critical review of consensus assessment methods for gesture elicitation",
      "authors": [
        "Additional Key Words",
        "Phrases"
      ],
      "year": 2022,
      "researchArea": "Mobile Device",
      "keywords": [],
      "citations": 0,
      "abstract": "HAL Id: hal-01788775 https://hal.science/hal-01788775v4  Submitted on 7 Feb 2022  HAL   is   a   multi-disciplinary   open   access archive for the deposit and dissemination of sci- entific research documents, whether they are pub- lished or not.   The documents may come from teaching and research i...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982055357_y5l1k823h.json"
    },
    {
      "id": "paper_1754982066817_5cz0sp9wp",
      "title": "2019 - Investigating unintended inputs for one-handed touch interaction beyond the touchscreen",
      "authors": [
        "Mobile Devices",
        "Services"
      ],
      "year": 2019,
      "researchArea": "Mobile Device",
      "keywords": [],
      "citations": 0,
      "abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  MobileHCI ’19, October 1–4, 2019, Taipei, Taiwan  ©   2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6825-4/19/10. . . $15.00 https://doi.org/10.1145/3338286.3340145  Figure 1: A participant interacting with a smartphone while the hand is being tracked by a motion capture system. ACM Reference Format:  Huy Viet Le, Sven Mayer, Benedict Steuerlein, and Niels Henze. 2019. Investigating Unintended Inputs for One-Handed Touch Inte- raction Beyond the Touchscreen. In   21st International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’19), October 1–4, 2019, Taipei, Taiwan.   ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3338286.3340145  1",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982066817_5cz0sp9wp.json"
    },
    {
      "id": "paper_1754982094921_f3nd5dvyu",
      "title": "2012 - Sleight of hand perception of finger motion from reduced marker sets",
      "authors": [
        "Graphics",
        "Vision",
        "Visualisation Group"
      ],
      "year": 2012,
      "researchArea": "Special Scenarios",
      "keywords": [
        "finger animation",
        "perception",
        "motion capture  ∗ e-mail:hoyetl@tcd.ie  † e-mail:ryallk@tcd.ie  ‡ e-mail:ramcdonn@cs.tcd.ie  § e-mail:carol.osullivan@cs.tcd.ie  1"
      ],
      "citations": 0,
      "abstract": "Copyright © 2012 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and ...",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982094921_f3nd5dvyu.json"
    },
    {
      "id": "paper_1754982098927_n06dpm1wz",
      "title": "2015 - Express it! An interactive system for visualizing expressiveness of conductor's gestures",
      "authors": [
        "Weight",
        "Space",
        "Time"
      ],
      "year": 2015,
      "researchArea": "Mobile Device",
      "keywords": [
        "Visualization",
        "expressivity",
        "conductor’s gestures",
        "gesture recognition",
        "music  ACM Classification Keywords  H."
      ],
      "citations": 0,
      "abstract": "A conductor provides a single unified vision of how to inter- pret and perform music. However, perceiving a conductor’s musical intention and expression is quite challenging as they convey information to performers with subtle, nuanced, and highly individualized gestures.   This artwork visualizes the conductor’s gestures in order to give the audience a better un- derstanding of its expressivity. To represent the expressivity of the gestures, we created motion profiles over eight frames, at 30 frames per second, and compared them to previously modeled gestures using three motion factors, called Weight, Space and Time from related concepts in Laban Movement Analysis (LMA). Based on this, we have created a real-time, interactive visualization that is driven by the motion factor pa- rameters. The visualization receives the input video stream, and it is transformed into a representation of the three motion factors extracted from the real-time conducting gestures.  Author",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982098927_n06dpm1wz.json"
    },
    {
      "id": "paper_1754982107742_dkul5elnv",
      "title": "2021 - StringTouch - from string instruments towards new interface morphologies",
      "authors": [
        "Flexible Screens",
        "Interaction Surfaces  Touch"
      ],
      "year": 2021,
      "researchArea": "Special Scenarios",
      "keywords": [
        "TUI",
        "String Instrument Inspired",
        "Interaction Vocabulary.  ACM Reference Format:  Beat Rossmy",
        "Sonja Rümelin",
        "and Alexander Wiethoff."
      ],
      "citations": 0,
      "abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  TEI ’21, February 14–17, 2021, Salzburg, Austria  ©   2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8213-7/21/02. . . $15.00 https://doi.org/10.1145/3430524.3440628",
      "doi": "",
      "hasFile": true,
      "hasThumbnail": true,
      "dataFile": "papers/paper_1754982107742_dkul5elnv.json"
    }
  ]
}